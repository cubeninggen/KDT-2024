{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SentenceClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_vocab,\n",
    "        hidden_dim,\n",
    "        embedding_dim,\n",
    "        n_layers,\n",
    "        dropout=0.5,\n",
    "        bidirectional=True,\n",
    "        model_type='lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding=nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "        if model_type=='rnn':\n",
    "            self.model=nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "\n",
    "            )\n",
    "        elif model_type=='lstm':\n",
    "            self.model=nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "\n",
    "            )\n",
    "        if bidirectional:\n",
    "            self.classifier=nn.Linear(hidden_dim*2,1)\n",
    "        else:\n",
    "            self.classifier=nn.Linear(hidden_dim*1)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,inputs):\n",
    "        embeddings=self.embedding(inputs)\n",
    "        output,_=self.model(embeddings)\n",
    "        last_output=output[:,-1,:]\n",
    "        last_output=self.dropout(last_output)\n",
    "        logits=self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-48\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\KDP-48\\Korpora\\nsmc\\ratings_test.txt\n",
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Korpora import Korpora\n",
    "\n",
    "corpus=Korpora.load('nsmc')\n",
    "corpus_df=pd.DataFrame(corpus.test)\n",
    "\n",
    "train=corpus_df.sample(frac=0.9,random_state=42)\n",
    "test=corpus_df.drop(train.index)\n",
    "\n",
    "print(train.head().to_markdown())\n",
    "print(f'Training Data Size : {len(train)}')\n",
    "print(f'Testing Data Size : {len(test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '<unk>', '.', '이', '영화', '의', '..', '가', '에', '...']\n",
      "5002\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "def build_vocab(corpus,n_vocab,special_tokens):\n",
    "    counter=Counter()\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    vocab=special_tokens\n",
    "    for token,count in counter.most_common(n_vocab):\n",
    "        vocab.append(token)\n",
    "    return vocab\n",
    "\n",
    "tokenizer=Okt()\n",
    "train_tokens=[tokenizer.morphs(review) for review in train.text]\n",
    "test_tokens=[tokenizer.morphs(review) for review in test.text]\n",
    "\n",
    "vocab=build_vocab(corpus=train_tokens,n_vocab=5000,special_tokens=['<pad>','<unk>'])\n",
    "token_to_id={token:idx for idx,token in enumerate(vocab)}\n",
    "id_to_token={token:idx for idx,token in enumerate(vocab)}\n",
    "\n",
    "print(vocab[:10])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 223 1716   10 4036 2095  193  755    4    2 2330 1031  220   26   13\n",
      " 4839    1    1    1    2    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n",
      "[3307    5 1997  456    8    1 1013 3906    5    1    1   13  223   51\n",
      "    3    1 4684    6    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def pad_sequences(sequences,max_length,pad_value):\n",
    "    result=list()\n",
    "    for sequence in sequences:\n",
    "        sequence=sequence[:max_length]\n",
    "        pad_length=max_length-len(sequence)\n",
    "        padded_sequence=sequence+[pad_value]*pad_length\n",
    "        result.append(padded_sequence)\n",
    "    return np.array(result)\n",
    "\n",
    "unk_id=token_to_id['<unk>']\n",
    "train_ids=[[token_to_id.get(token,unk_id) for token in review] for review in train_tokens]\n",
    "test_ids=[[token_to_id.get(token,unk_id) for token in review] for review in test_tokens]\n",
    "\n",
    "max_length=32\n",
    "pad_id=token_to_id['<pad>']\n",
    "train_ids=pad_sequences(train_ids,max_length,pad_id)\n",
    "test_ids=pad_sequences(test_ids,max_length,pad_id)\n",
    "\n",
    "print(train_ids[0])\n",
    "print(test_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KDP-48\\AppData\\Local\\Temp\\ipykernel_7356\\577776302.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_ids=torch.tensor(train_ids)\n",
      "C:\\Users\\KDP-48\\AppData\\Local\\Temp\\ipykernel_7356\\577776302.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_ids=torch.tensor(test_ids)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "train_ids=torch.tensor(train_ids)\n",
    "test_ids=torch.tensor(test_ids)\n",
    "\n",
    "train_labels=torch.tensor(train.label.values,dtype=torch.float32)\n",
    "test_labels=torch.tensor(test.label.values,dtype=torch.float32)\n",
    "\n",
    "train_dataset=TensorDataset(train_ids,train_labels)\n",
    "test_dataset=TensorDataset(test_ids,test_labels)\n",
    "\n",
    "train_loader=DataLoader(train_dataset,batch_size=16,shuffle=True)\n",
    "test_loader=DataLoader(test_dataset,batch_size=16,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "n_vocab=len(token_to_id)\n",
    "hidden_dim=64\n",
    "embedding_dim=128\n",
    "n_layer=2\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "classifier=SentenceClassifier(n_vocab=n_vocab,hidden_dim=hidden_dim,embedding_dim=embedding_dim,n_layers=n_layer).to(device)\n",
    "criterion=nn.BCEWithLogitsLoss().to(device)\n",
    "optimizer=optim.RMSprop(classifier.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss 0 : 0.7053079605102539\n",
      "Train Loss 500 : 0.6940382004022122\n",
      "Train Loss 1000 : 0.6918097991090674\n",
      "Train Loss 1500 : 0.6760961227342337\n",
      "Train Loss 2000 : 0.6632575097976476\n",
      "Train Loss 2500 : 0.6533547847068868\n",
      "Val Loss : 0.591471304813513, Val Accuracy : 0.6904\n",
      "Train Loss 0 : 0.3950698673725128\n",
      "Train Loss 500 : 0.5772097259462474\n",
      "Train Loss 1000 : 0.5633098314573001\n",
      "Train Loss 1500 : 0.5500267355700956\n",
      "Train Loss 2000 : 0.5377030125830783\n",
      "Train Loss 2500 : 0.5225558206945455\n",
      "Val Loss : 0.45505432165659276, Val Accuracy : 0.7902\n",
      "Train Loss 0 : 0.5615537762641907\n",
      "Train Loss 500 : 0.4238381556973248\n",
      "Train Loss 1000 : 0.4194644880104256\n",
      "Train Loss 1500 : 0.41523321999918217\n",
      "Train Loss 2000 : 0.41394984914266364\n",
      "Train Loss 2500 : 0.4120623234276627\n",
      "Val Loss : 0.419062857144176, Val Accuracy : 0.8056\n",
      "Train Loss 0 : 0.3411809206008911\n",
      "Train Loss 500 : 0.36751887841733866\n",
      "Train Loss 1000 : 0.36638016069119983\n",
      "Train Loss 1500 : 0.3658599347561141\n",
      "Train Loss 2000 : 0.367274000734344\n",
      "Train Loss 2500 : 0.36884112052413903\n",
      "Val Loss : 0.40760253941098723, Val Accuracy : 0.8152\n",
      "Train Loss 0 : 0.4969368577003479\n",
      "Train Loss 500 : 0.3284528364321429\n",
      "Train Loss 1000 : 0.3205758472988298\n",
      "Train Loss 1500 : 0.3240679486503925\n",
      "Train Loss 2000 : 0.32709705547384477\n",
      "Train Loss 2500 : 0.32903913373216204\n",
      "Val Loss : 0.4021430875356205, Val Accuracy : 0.8208\n"
     ]
    }
   ],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()\n",
    "    losses = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f\"Train Loss {step} : {np.mean(losses)}\")\n",
    "\n",
    "\n",
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()\n",
    "    losses = list()\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits)>.5\n",
    "        corrects.extend(\n",
    "            torch.eq(yhat, labels).cpu().tolist()\n",
    "        )\n",
    "\n",
    "    print(f\"Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}\")\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(classifier, train_loader, criterion, optimizer, device, interval)\n",
    "    test(classifier, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "보고싶다 [ 2.6081896e-01  8.5469204e-01  6.4059186e-01  5.1611364e-01\n",
      "  8.9949590e-01  8.6668253e-01 -1.1371868e+00  2.6883996e+00\n",
      "  8.2390442e-02  9.7361249e-01  7.6103401e-01 -1.1177559e+00\n",
      " -8.0383009e-01  4.0308902e-01 -9.3939967e-02  5.7157969e-01\n",
      "  1.0288697e+00  1.0952957e+00 -7.3642898e-01  9.5771170e-01\n",
      "  3.2925922e-01  2.5472185e-01 -7.2439611e-01  8.5415013e-02\n",
      " -5.6072164e-01 -3.4131128e-02 -2.7754493e+00 -4.9875382e-01\n",
      " -3.6690271e-01  1.1390558e+00  8.0363810e-01  5.3548390e-01\n",
      "  3.6653644e-01  8.4745169e-01  2.5196271e+00  7.6317692e-01\n",
      "  5.8053356e-01  2.8422257e-01 -2.2621222e-01 -7.8375906e-01\n",
      " -4.3386695e-01 -1.7232476e-01  2.0026830e-01  6.3932067e-01\n",
      "  4.9875519e-01  2.2426569e-01  6.9094472e-02 -2.0751071e+00\n",
      " -9.0699661e-01  3.3767518e-01  2.5023568e+00 -8.8527930e-01\n",
      " -6.9362736e-01 -9.5260102e-01  1.3477335e+00 -6.7588037e-01\n",
      " -1.6966709e-01 -1.1982466e+00  6.3574862e-01  5.2357918e-01\n",
      " -1.6391348e+00 -7.8939593e-01  5.1783490e-01 -9.6359563e-01\n",
      " -1.2824370e+00  4.9169773e-01 -8.9553511e-01 -1.9362224e+00\n",
      " -8.8342750e-01  3.7900090e-01  5.6643265e-01  1.5262482e-02\n",
      " -1.6389257e-01  3.8223347e-01  6.1503917e-01  1.2657839e+00\n",
      " -1.3238321e-01  3.3915359e-01 -1.4225852e+00  8.0893028e-01\n",
      " -1.0301361e+00  8.6297327e-01 -2.0514235e-02  1.3566132e+00\n",
      " -5.5870891e-01  4.9001762e-01 -5.4004401e-01 -1.0757897e+00\n",
      " -6.3215649e-01  8.5125071e-01 -9.7450984e-01  1.0675310e+00\n",
      "  3.6821029e-01 -2.3683996e+00 -4.6619478e-01  1.1170998e+00\n",
      "  1.1963625e+00  1.9212140e+00  1.7075887e-01 -5.3615463e-01\n",
      "  5.6344721e-02  5.0178695e-01  8.5241950e-01 -1.0578520e+00\n",
      "  1.8158164e-03 -3.3495548e+00  6.0968274e-01  1.3950464e+00\n",
      " -4.5209563e-01  1.1585561e+00  1.1514860e+00  2.5848562e-01\n",
      " -1.2140508e+00  5.5327165e-01  2.0522398e-01 -1.5628445e+00\n",
      "  8.2089877e-01 -8.6443013e-01 -2.0022440e+00  1.1844556e+00\n",
      "  6.3721311e-01  5.9432060e-01  7.9541498e-01 -1.0230350e+00\n",
      " -3.8659936e-01 -5.6407887e-01 -5.7597321e-01 -2.3517084e+00]\n"
     ]
    }
   ],
   "source": [
    "token_to_embedding = dict()\n",
    "embedding_matrix = classifier.embedding.weight.detach().cpu().numpy()\n",
    "\n",
    "for word, emb in zip(vocab, embedding_matrix):\n",
    "    token_to_embedding[word] = emb\n",
    "\n",
    "token = vocab[1000]\n",
    "print(token, token_to_embedding[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TORCH_TEXT_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
